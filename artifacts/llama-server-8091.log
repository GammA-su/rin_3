Starting llama.cpp server: /home/alpahos/Documents/gptoss120/llama.cpp/build/bin/llama-server -m /home/alpahos/8to/models/gpt-oss-20b/gpt-oss-20b-Q4_K_M.gguf -ngl 99 --main-gpu 0 -c 4096 -t 16 -b 1024 --host 0.0.0.0 --port 8091 -ub 512 --flash-attn on
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
build: 6356 (9961d244) with cc (GCC) 15.2.1 20250813 for x86_64-pc-linux-gnu
system info: n_threads = 16, n_threads_batch = 16, total_threads = 16

system_info: n_threads = 16 (n_threads_batch = 16) / 16 | CUDA : ARCHS = 890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

main: binding port with default address family
main: HTTP server is listening, hostname: 0.0.0.0, port: 8091, http threads: 15
main: loading model
srv    load_model: loading model '/home/alpahos/8to/models/gpt-oss-20b/gpt-oss-20b-Q4_K_M.gguf'
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4090) - 22352 MiB free
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /home/alpahos/8to/models/gpt-oss-20b/gpt-oss-20b-Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  22:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  23:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  24:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  25: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  26:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  27:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  28:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  30:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - kv  35:               general.quantization_version u32              = 2
llama_model_loader: - kv  36:                          general.file_type u32              = 15
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type q5_0:   61 tensors
llama_model_loader: - type q8_0:   13 tensors
llama_model_loader: - type q4_K:   24 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 10.81 GiB (4.44 BPW) 
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch             = gpt-oss
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 2880
print_info: n_layer          = 24
print_info: n_head           = 64
print_info: n_head_kv        = 8
print_info: n_rot            = 64
print_info: n_swa            = 128
print_info: is_swa_any       = 1
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 2880
print_info: n_expert         = 32
print_info: n_expert_used    = 4
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = yarn
print_info: freq_base_train  = 150000.0
print_info: freq_scale_train = 0.03125
print_info: n_ctx_orig_yarn  = 4096
print_info: rope_finetuned   = unknown
print_info: model type       = 20B
print_info: model params     = 20.91 B
print_info: general.name     = Gpt-Oss-20B
print_info: n_ff_exp         = 2880
print_info: vocab type       = BPE
print_info: n_vocab          = 201088
print_info: n_merges         = 446189
print_info: BOS token        = 199998 '<|startoftext|>'
print_info: EOS token        = 200002 '<|return|>'
print_info: EOT token        = 199999 '<|endoftext|>'
print_info: PAD token        = 200017 '<|reserved_200017|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 199999 '<|endoftext|>'
print_info: EOG token        = 200002 '<|return|>'
print_info: EOG token        = 200012 '<|call|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
srv  log_server_r: request: POST /completion 127.0.0.1 503
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 10694.15 MiB
load_tensors:   CPU_Mapped model buffer size =   379.71 MiB
......................srv  log_server_r: request: POST /completion 127.0.0.1 503
.........................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.77 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 4096 cells
llama_kv_cache:      CUDA0 KV buffer size =    96.00 MiB
llama_kv_cache: size =   96.00 MiB (  4096 cells,  12 layers,  1/1 seqs), K (f16):   48.00 MiB, V (f16):   48.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 768 cells
llama_kv_cache:      CUDA0 KV buffer size =    18.00 MiB
llama_kv_cache: size =   18.00 MiB (   768 cells,  12 layers,  1/1 seqs), K (f16):    9.00 MiB, V (f16):    9.00 MiB
llama_context:      CUDA0 compute buffer size =   398.38 MiB
llama_context:  CUDA_Host compute buffer size =    15.15 MiB
llama_context: graph nodes  = 1352
llama_context: graph splits = 2
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|return|> logit bias = -inf
common_init_from_params: added <|call|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv          init: initializing slots, n_slots = 1
slot         init: id  0 | task -1 | new slot n_ctx_slot = 4096
main: model loaded
main: chat template, chat_template: {# Chat template fixes by Unsloth #}
{#-
  In addition to the normal inputs of `messages` and `tools`, this template also accepts the
  following kwargs:
  - "builtin_tools": A list, can contain "browser" and/or "python".
  - "model_identity": A string that optionally describes the model identity.
  - "reasoning_effort": A string that describes the reasoning effort, defaults to "medium".
 #}

{#- Tool Definition Rendering ============================================== #}
{%- macro render_typescript_type(param_spec, required_params, is_nullable=false) -%}
    {%- if param_spec.type == "array" -%}
        {%- if param_spec['items'] -%}
            {%- if param_spec['items']['type'] == "string" -%}
                {{- "string[]" }}
            {%- elif param_spec['items']['type'] == "number" -%}
                {{- "number[]" }}
            {%- elif param_spec['items']['type'] == "integer" -%}
                {{- "number[]" }}
            {%- elif param_spec['items']['type'] == "boolean" -%}
                {{- "boolean[]" }}
            {%- else -%}
                {%- set inner_type = render_typescript_type(param_spec['items'], required_params) -%}
                {%- if inner_type == "object | object" or inner_type|length > 50 -%}
                    {{- "any[]" }}
                {%- else -%}
                    {{- inner_type + "[]" }}
                {%- endif -%}
            {%- endif -%}
            {%- if param_spec.nullable -%}
                {{- " | null" }}
            {%- endif -%}
        {%- else -%}
            {{- "any[]" }}
            {%- if param_spec.nullable -%}
                {{- " | null" }}
            {%- endif -%}
        {%- endif -%}
    {%- elif param_spec.type is defined and param_spec.type is iterable and param_spec.type is not string and param_spec.type is not mapping and param_spec.type[0] is defined -%}
        {#- Handle array of types like ["object", "object"] from Union[dict, list] #}
        {%- if param_spec.type | length > 1 -%}
            {{- param_spec.type | join(" | ") }}
        {%- else -%}
            {{- param_spec.type[0] }}
        {%- endif -%}
    {%- elif param_spec.oneOf -%}
        {#- Handle oneOf schemas - check for complex unions and fallback to any #}
        {%- set has_object_variants = false -%}
        {%- for variant in param_spec.oneOf -%}
            {%- if variant.type == "object" -%}
                {%- set has_object_variants = true -%}
            {%- endif -%}
        {%- endfor -%}
        {%- if has_object_variants and param_spec.oneOf|length > 1 -%}
            {{- "any" }}
        {%- else -%}
            {%- for variant in param_spec.oneOf -%}
                {{- render_typescript_type(variant, required_params) -}}
                {%- if variant.description %}
                    {{- "// " + variant.description }}
                {%- endif -%}
                {%- if variant.default is defined %}
                    {{ "// default: " + variant.default|tojson }}
                {%- endif -%}
                {%- if not loop.last %}
                    {{- " | " }}
                {% endif -%}
            {%- endfor -%}
        {%- endif -%}
    {%- elif param_spec.type == "string" -%}
        {%- if param_spec.enum -%}
            {{- '"' + param_spec.enum|join('" | "') + '"' -}}
        {%- else -%}
            {{- "string" }}
            {%- if param_spec.nullable %}
                {{- " | null" }}
            {%- endif -%}
        {%- endif -%}
    {%- elif param_spec.type == "number" -%}
        {{- "number" }}
    {%- elif param_spec.type == "integer" -%}
        {{- "number" }}
    {%- elif param_spec.type == "boolean" -%}
        {{- "boolean" }}

    {%- elif param_spec.type == "object" -%}
        {%- if param_spec.properties -%}
            {{- "{\n" }}
            {%- for prop_name, prop_spec in param_spec.properties.items() -%}
                {{- prop_name -}}
                {%- if prop_name not in (param_spec.required or []) -%}
                    {{- "?" }}
                {%- endif -%}
                {{- ": " }}
                {{ render_typescript_type(prop_spec, param_spec.required or []) }}
                {%- if not loop.last -%}
                    {{-", " }}
                {%- endif -%}
            {%- endfor -%}
            {{- "}" }}
        {%- else -%}
            {{- "object" }}
        {%- endif -%}
    {%- else -%}
        {{- "any" }}
    {%- endif -%}
{%- endmacro -%}

{%- macro render_tool_namespace(namespace_name, tools) -%}
    {{- "## " + namespace_name + "\n\n" }}
    {{- "namespace " + namespace_name + " {\n\n" }}
    {%- for tool in tools %}
        {%- set tool = tool.function %}
        {{- "// " + tool.description + "\n" }}
        {{- "type "+ tool.name + " = " }}
        {%- if tool.parameters and tool.parameters.properties %}
            {{- "(_: {\n" }}
            {%- for param_name, param_spec in tool.parameters.properties.items() %}
                {%- if param_spec.description %}
                    {{- "// " + param_spec.description + "\n" }}
                {%- endif %}
                {{- param_name }}
                {%- if param_name not in (tool.parameters.required or []) -%}
                    {{- "?" }}
                {%- endif -%}
                {{- ": " }}
                {{- render_typescript_type(param_spec, tool.parameters.required or []) }}
                {%- if param_spec.default is defined -%}
                    {%- if param_spec.enum %}
                        {{- ", // default: " + param_spec.default }}
                    {%- elif param_spec.oneOf %}
                        {{- "// default: " + param_spec.default }}
                    {%- else %}
                        {{- ", // default: " + param_spec.default|tojson }}
                    {%- endif -%}
                {%- endif -%}
                {%- if not loop.last %}
                    {{- ",\n" }}
                {%- else %}
                    {{- ",\n" }}
                {%- endif -%}
            {%- endfor %}
            {{- "}) => any;\n\n" }}
        {%- else -%}
            {{- "() => any;\n\n" }}
        {%- endif -%}
    {%- endfor %}
    {{- "} // namespace " + namespace_name }}
{%- endmacro -%}

{%- macro render_builtin_tools(browser_tool, python_tool) -%}
    {%- if browser_tool %}
        {{- "## browser\n\n" }}
        {{- "// Tool for browsing.\n" }}
        {{- "// The `cursor` appears in brackets before each browsing display: `[{cursor}]`.\n" }}
        {{- "// Cite information from the tool using the following format:\n" }}
        {{- "// `【{cursor}†L{line_start}(-L{line_end})?】`, for example: `【6†L9-L11】` or `【8†L3】`.\n" }}
        {{- "// Do not quote more than 10 words directly from the tool output.\n" }}
        {{- "// sources=web (default: web)\n" }}
        {{- "namespace browser {\n\n" }}
        {{- "// Searches for information related to `query` and displays `topn` results.\n" }}
        {{- "type search = (_: {\n" }}
        {{- "query: string,\n" }}
        {{- "topn?: number, // default: 10\n" }}
        {{- "source?: string,\n" }}
        {{- "}) => any;\n\n" }}
        {{- "// Opens the link `id` from the page indicated by `cursor` starting at line number `loc`, showing `num_lines` lines.\n" }}
        {{- "// Valid link ids are displayed with the formatting: `【{id}†.*】`.\n" }}
        {{- "// If `cursor` is not provided, the most recent page is implied.\n" }}
        {{- "// If `id` is a string, it is treated as a fully qualified URL associated with `source`.\n" }}
        {{- "// If `loc` is not provided, the viewport will be positioned at the beginning of the document or centered on the most relevant passage, if available.\n" }}
        {{- "// Use this function without `id` to scroll to a new location of an opened page.\n" }}
        {{- "type open = (_: {\n" }}
        {{- "id?: number | string, // default: -1\n" }}
        {{- "cursor?: number, // default: -1\n" }}
        {{- "loc?: number, // default: -1\n" }}
        {{- "num_lines?: number, // default: -1\n" }}
        {{- "view_source?: boolean, // default: false\n" }}
        {{- "source?: string,\n" }}
        {{- "}) => any;\n\n" }}
        {{- "// Finds exact matches of `pattern` in the current page, or the page given by `cursor`.\n" }}
        {{- "type find = (_: {\n" }}
        {{- "pattern: string,\n" }}
        {{- "cursor?: number, // default: -1\n" }}
        {{- "}) => any;\n\n" }}
        {{- "} // namespace browser\n\n" }}
    {%- endif -%}

    {%- if python_tool %}
        {{- "## python\n\n" }}
        {{- "Use this tool to execute Python code in your chain of thought. The code will not be shown to the user. This tool should be used for internal reasoning, but not for code that is intended to be visible to the user (e.g. when creating plots, tables, or files).\n\n" }}
        {{- "When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 120.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is UNKNOWN. Depends on the cluster.\n\n" }}
    {%- endif -%}
{%- endmacro -%}

{#- System Message Construction ============================================ #}
{%- macro build_system_message() -%}
    {%- if model_identity is not defined %}
        {%- set model_identity = "You are ChatGPT, a large language model trained by OpenAI." %}
    {%- endif %}
    {{- model_identity + "\n" }}
    {{- "Knowledge cutoff: 2024-06\n" }}
    {{- "Current date: " + strftime_now("%Y-%m-%d") + "\n\n" }}
    {%- if reasoning_effort is not defined %}
        {%- set reasoning_effort = "medium" %}
    {%- endif %}
    {{- "Reasoning: " + reasoning_effort + "\n\n" }}
    {%- if builtin_tools is defined and builtin_tools is not none %}
        {{- "# Tools\n\n" }}
        {%- set available_builtin_tools = namespace(browser=false, python=false) %}
        {%- for tool in builtin_tools %}
            {%- if tool == "browser" %}
                {%- set available_builtin_tools.browser = true %}
            {%- elif tool == "python" %}
                {%- set available_builtin_tools.python = true %}
            {%- endif %}
        {%- endfor %}
        {{- render_builtin_tools(available_builtin_tools.browser, available_builtin_tools.python) }}
    {%- endif -%}
    {{- "# Valid channels: analysis, commentary, final. Channel must be included for every message." }}
    {%- if tools -%}
        {{- "\nCalls to these tools must go to the commentary channel: 'functions'." }}
    {%- endif -%}
{%- endmacro -%}

{#- Main Template Logic ================================================= #}
{#- Set defaults #}

{#- Render system message #}
{{- "<|start|>system<|message|>" }}
{{- build_system_message() }}
{{- "<|end|>" }}

{#- Extract developer message #}
{%- if developer_instructions is defined and developer_instructions is not none %}
    {%- set developer_message = developer_instructions %}
    {%- set loop_messages = messages %}
{%- elif messages[0].role == "developer" or messages[0].role == "system" %}
    {%- set developer_message = messages[0].content %}
    {%- set loop_messages = messages[1:] %}
{%- else %}
    {%- set developer_message = "" %}
    {%- set loop_messages = messages %}
{%- endif %}

{#- Render developer message #}
{%- if developer_message or tools %}
    {{- "<|start|>developer<|message|>" }}
    {%- if developer_message %}
        {{- "# Instructions\n\n" }}
        {{- developer_message }}
    {%- endif %}
    {%- if tools -%}
        {%- if developer_message %}
            {{- "\n\n" }}
        {%- endif %}
        {{- "# Tools\n\n" }}
        {{- render_tool_namespace("functions", tools) }}
    {%- endif -%}
    {{- "<|end|>" }}
{%- endif %}

{#- Render messages #}
{%- set last_tool_call = namespace(name=none) %}
{%- for message in loop_messages -%}
    {#- At this point only assistant/user/tool messages should remain #}
    {%- if message.role == 'assistant' -%}
        {#- Checks to ensure the messages are being passed in the format we expect #}
        {%- if "thinking" in message %}
            {%- if "<|channel|>analysis<|message|>" in message.thinking or "<|channel|>final<|message|>" in message.thinking %}
                {{- raise_exception("You have passed a message containing <|channel|> tags in the thinking field. Instead of doing this, you should pass analysis messages (the string between '<|message|>' and '<|end|>') in the 'thinking' field, and final messages (the string between '<|message|>' and '<|end|>') in the 'content' field.") }}
            {%- endif %}
        {%- endif %}
        {%- if "tool_calls" in message %}
            {#- We need very careful handling here - we want to drop the tool call analysis message if the model #}
            {#- has output a later <|final|> message, but otherwise we want to retain it. This is the only case #}
            {#- when we render CoT/analysis messages in inference. #}
            {%- set future_final_message = namespace(found=false) %}
            {%- for future_message in loop_messages[loop.index:] %}
                {%- if future_message.role == 'assistant' and "tool_calls" not in future_message %}
                    {%- set future_final_message.found = true %}
                {%- endif %}
            {%- endfor %}
            {#- We assume max 1 tool call per message, and so we infer the tool call name #}
            {#- in "tool" messages from the most recent assistant tool call name #}
            {%- set tool_call = message.tool_calls[0] %}
            {%- if tool_call.function %}
                {%- set tool_call = tool_call.function %}
            {%- endif %}
            {%- if message.content and message.thinking %}
                {{- raise_exception("Cannot pass both content and thinking in an assistant message with tool calls! Put the analysis message in one or the other, but not both.") }}
            {%- elif message.content and not future_final_message.found %}
                {{- "<|start|>assistant<|channel|>analysis<|message|>" + message.content + "<|end|>" }}
            {%- elif message.thinking and not future_final_message.found %}
                {{- "<|start|>assistant<|channel|>analysis<|message|>" + message.thinking + "<|end|>" }}
            {%- endif %}
            {{- "<|start|>assistant to=" }}
            {{- "functions." + tool_call.name + "<|channel|>commentary " }}
            {{- (tool_call.content_type if tool_call.content_type is defined else "json") + "<|message|>" }}
            {%- if tool_call.arguments is string %}
                {{- tool_call.arguments }}
            {%- else %}
                {{- tool_call.arguments|tojson }}
            {%- endif %}
            {{- "<|call|>" }}
            {%- set last_tool_call.name = tool_call.name %}
        {%- elif loop.last and not add_generation_prompt %}
            {#- Only render the CoT if the final turn is an assistant turn and add_generation_prompt is false #}
            {#- This is a situation that should only occur in training, never in inference. #}
            {%- if "thinking" in message %}
                {{- "<|start|>assistant<|channel|>analysis<|message|>" + message.thinking + "<|end|>" }}
            {%- endif %}
            {#- <|return|> indicates the end of generation, but <|end|> does not #}
            {#- <|return|> should never be an input to the model, but we include it as the final token #}
            {#- when training, so the model learns to emit it. #}
            {{- "<|start|>assistant<|channel|>final<|message|>" + message.content + "<|end|>" }}
        {%- elif "thinking" in message %}
            {#- CoT is dropped during all previous turns, so we never render it for inference #}
            {{- "<|start|>assistant<|channel|>analysis<|message|>" + message.content + "<|end|>" }}
            {%- set last_tool_call.name = none %}
        {%- else %}
            {#- CoT is dropped during all previous turns, so we never render it for inference #}
            {{- "<|start|>assistant<|channel|>final<|message|>" + message.content + "<|end|>" }}
            {%- set last_tool_call.name = none %}
        {%- endif %}
    {%- elif message.role == 'tool' -%}
        {%- if last_tool_call.name is none %}
            {{- raise_exception("Message has tool role, but there was no previous assistant message with a tool call!") }}
        {%- endif %}
        {{- "<|start|>functions." + last_tool_call.name }}
        {%- if message.content is string %}
            {{- " to=assistant<|channel|>commentary<|message|>" + message.content + "<|end|>" }}
        {%- else %}
            {{- " to=assistant<|channel|>commentary<|message|>" + message.content|tojson + "<|end|>" }}
        {%- endif %}
    {%- elif message.role == 'user' -%}
        {{- "<|start|>user<|message|>" + message.content + "<|end|>" }}
    {%- endif -%}
{%- endfor -%}

{#- Generation prompt #}
{%- if add_generation_prompt -%}
<|start|>assistant
{%- endif -%}
{# Copyright 2025-present Unsloth. Apache 2.0 License. Unsloth chat template fixes. Edited from ggml-org & OpenAI #}, example_format: '<|start|>system<|message|>You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|message|>Hi there<|return|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: server is listening on http://0.0.0.0:8091 - starting the main loop
srv  update_slots: all slots are idle
slot launch_slot_: id  0 | task 0 | processing task
slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 1
slot update_slots: id  0 | task 0 | kv cache rm [0, end)
slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 1, n_tokens = 1, progress = 1.000000
slot update_slots: id  0 | task 0 | prompt done, n_past = 1, n_tokens = 1
slot update_slots: id  0 | task 0 | SWA checkpoint create, pos_min = 0, pos_max = 0, size = 0.024 MiB, total = 1/3 (0.024 MiB)
slot      release: id  0 | task 0 | stop processing: n_past = 1, truncated = 0
slot print_timing: id  0 | task 0 | 
prompt eval time =      15.35 ms /     1 tokens (   15.35 ms per token,    65.16 tokens per second)
       eval time =       0.01 ms /     1 tokens (    0.01 ms per token, 142857.14 tokens per second)
      total time =      15.35 ms /     2 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
slot launch_slot_: id  0 | task 2 | processing task
slot update_slots: id  0 | task 2 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 17
slot update_slots: id  0 | task 2 | kv cache rm [0, end)
slot update_slots: id  0 | task 2 | prompt processing progress, n_past = 17, n_tokens = 17, progress = 1.000000
slot update_slots: id  0 | task 2 | prompt done, n_past = 17, n_tokens = 17
slot update_slots: id  0 | task 2 | SWA checkpoint create, pos_min = 0, pos_max = 16, size = 0.399 MiB, total = 2/3 (0.423 MiB)
slot      release: id  0 | task 2 | stop processing: n_past = 48, truncated = 0
slot print_timing: id  0 | task 2 | 
prompt eval time =     175.06 ms /    17 tokens (   10.30 ms per token,    97.11 tokens per second)
       eval time =     160.75 ms /    32 tokens (    5.02 ms per token,   199.07 tokens per second)
      total time =     335.81 ms /    49 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
slot launch_slot_: id  0 | task 35 | processing task
slot update_slots: id  0 | task 35 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 2049
slot update_slots: id  0 | task 35 | kv cache rm [0, end)
slot update_slots: id  0 | task 35 | prompt processing progress, n_past = 1024, n_tokens = 1024, progress = 0.499756
slot update_slots: id  0 | task 35 | kv cache rm [1024, end)
slot update_slots: id  0 | task 35 | prompt processing progress, n_past = 2048, n_tokens = 1024, progress = 0.999512
slot update_slots: id  0 | task 35 | kv cache rm [2048, end)
slot update_slots: id  0 | task 35 | prompt processing progress, n_past = 2049, n_tokens = 1, progress = 1.000000
slot update_slots: id  0 | task 35 | prompt done, n_past = 2049, n_tokens = 1
slot update_slots: id  0 | task 35 | SWA checkpoint create, pos_min = 1281, pos_max = 2048, size = 18.009 MiB, total = 3/3 (18.432 MiB)
slot      release: id  0 | task 35 | stop processing: n_past = 2056, truncated = 0
slot print_timing: id  0 | task 35 | 
prompt eval time =     265.52 ms /  2049 tokens (    0.13 ms per token,  7716.93 tokens per second)
       eval time =      39.81 ms /     8 tokens (    4.98 ms per token,   200.94 tokens per second)
      total time =     305.33 ms /  2057 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
slot launch_slot_: id  0 | task 46 | processing task
slot update_slots: id  0 | task 46 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 2049
slot update_slots: id  0 | task 46 | need to evaluate at least 1 token for each active slot, n_past = 2049, n_prompt_tokens = 2049
slot update_slots: id  0 | task 46 | kv cache rm [2048, end)
slot update_slots: id  0 | task 46 | prompt processing progress, n_past = 2049, n_tokens = 1, progress = 0.000488
slot update_slots: id  0 | task 46 | prompt done, n_past = 2049, n_tokens = 1
slot update_slots: id  0 | task 46 | SWA checkpoint erase, pos_min = 1281, pos_max = 2048, size = 18.009 MiB
slot update_slots: id  0 | task 46 | SWA checkpoint create, pos_min = 1288, pos_max = 2048, size = 17.845 MiB, total = 3/3 (36.253 MiB)
slot      release: id  0 | task 46 | stop processing: n_past = 2056, truncated = 0
slot print_timing: id  0 | task 46 | 
prompt eval time =       8.64 ms /     1 tokens (    8.64 ms per token,   115.78 tokens per second)
       eval time =      39.55 ms /     8 tokens (    4.94 ms per token,   202.26 tokens per second)
      total time =      48.19 ms /     9 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
slot launch_slot_: id  0 | task 55 | processing task
slot update_slots: id  0 | task 55 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 2049
slot update_slots: id  0 | task 55 | need to evaluate at least 1 token for each active slot, n_past = 2049, n_prompt_tokens = 2049
slot update_slots: id  0 | task 55 | kv cache rm [2048, end)
slot update_slots: id  0 | task 55 | prompt processing progress, n_past = 2049, n_tokens = 1, progress = 0.000488
slot update_slots: id  0 | task 55 | prompt done, n_past = 2049, n_tokens = 1
slot update_slots: id  0 | task 55 | SWA checkpoint erase, pos_min = 1288, pos_max = 2048, size = 17.845 MiB
slot update_slots: id  0 | task 55 | SWA checkpoint create, pos_min = 1288, pos_max = 2048, size = 17.845 MiB, total = 3/3 (53.699 MiB)
slot      release: id  0 | task 55 | stop processing: n_past = 2056, truncated = 0
slot print_timing: id  0 | task 55 | 
prompt eval time =       9.58 ms /     1 tokens (    9.58 ms per token,   104.35 tokens per second)
       eval time =      38.27 ms /     8 tokens (    4.78 ms per token,   209.04 tokens per second)
      total time =      47.85 ms /     9 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
slot launch_slot_: id  0 | task 64 | processing task
slot update_slots: id  0 | task 64 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 2049
slot update_slots: id  0 | task 64 | need to evaluate at least 1 token for each active slot, n_past = 2049, n_prompt_tokens = 2049
slot update_slots: id  0 | task 64 | kv cache rm [2048, end)
slot update_slots: id  0 | task 64 | prompt processing progress, n_past = 2049, n_tokens = 1, progress = 0.000488
slot update_slots: id  0 | task 64 | prompt done, n_past = 2049, n_tokens = 1
slot update_slots: id  0 | task 64 | SWA checkpoint erase, pos_min = 1288, pos_max = 2048, size = 17.845 MiB
slot update_slots: id  0 | task 64 | SWA checkpoint create, pos_min = 1288, pos_max = 2048, size = 17.845 MiB, total = 3/3 (53.535 MiB)
slot      release: id  0 | task 64 | stop processing: n_past = 2056, truncated = 0
slot print_timing: id  0 | task 64 | 
prompt eval time =       9.46 ms /     1 tokens (    9.46 ms per token,   105.65 tokens per second)
       eval time =      38.55 ms /     8 tokens (    4.82 ms per token,   207.52 tokens per second)
      total time =      48.02 ms /     9 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
slot launch_slot_: id  0 | task 73 | processing task
slot update_slots: id  0 | task 73 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 4097
slot      release: id  0 | task 73 | stop processing: n_past = 0, truncated = 0
srv    send_error: task id = 73, error: the request exceeds the available context size. try increasing the context size or enable context shift
srv  update_slots: no tokens to decode
srv  update_slots: all slots are idle
srv  cancel_tasks: cancel task, id_task = 73
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 400
slot launch_slot_: id  0 | task 76 | processing task
slot update_slots: id  0 | task 76 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 3073
slot update_slots: id  0 | task 76 | kv cache rm [2048, end)
slot update_slots: id  0 | task 76 | prompt processing progress, n_past = 3072, n_tokens = 1024, progress = 0.333225
slot update_slots: id  0 | task 76 | kv cache rm [3072, end)
slot update_slots: id  0 | task 76 | prompt processing progress, n_past = 3073, n_tokens = 1, progress = 0.333550
slot update_slots: id  0 | task 76 | prompt done, n_past = 3073, n_tokens = 1
slot update_slots: id  0 | task 76 | SWA checkpoint erase, pos_min = 1288, pos_max = 2048, size = 17.845 MiB
slot update_slots: id  0 | task 76 | SWA checkpoint create, pos_min = 2305, pos_max = 3072, size = 18.009 MiB, total = 3/3 (53.699 MiB)
slot      release: id  0 | task 76 | stop processing: n_past = 3080, truncated = 0
slot print_timing: id  0 | task 76 | 
prompt eval time =     150.60 ms /  1025 tokens (    0.15 ms per token,  6806.29 tokens per second)
       eval time =      40.67 ms /     8 tokens (    5.08 ms per token,   196.70 tokens per second)
      total time =     191.27 ms /  1033 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
slot launch_slot_: id  0 | task 86 | processing task
slot update_slots: id  0 | task 86 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 3073
slot update_slots: id  0 | task 86 | need to evaluate at least 1 token for each active slot, n_past = 3073, n_prompt_tokens = 3073
slot update_slots: id  0 | task 86 | kv cache rm [3072, end)
slot update_slots: id  0 | task 86 | prompt processing progress, n_past = 3073, n_tokens = 1, progress = 0.000325
slot update_slots: id  0 | task 86 | prompt done, n_past = 3073, n_tokens = 1
slot update_slots: id  0 | task 86 | SWA checkpoint erase, pos_min = 2305, pos_max = 3072, size = 18.009 MiB
slot update_slots: id  0 | task 86 | SWA checkpoint create, pos_min = 2312, pos_max = 3072, size = 17.845 MiB, total = 3/3 (53.699 MiB)
slot      release: id  0 | task 86 | stop processing: n_past = 3080, truncated = 0
slot print_timing: id  0 | task 86 | 
prompt eval time =       9.82 ms /     1 tokens (    9.82 ms per token,   101.87 tokens per second)
       eval time =      42.52 ms /     8 tokens (    5.32 ms per token,   188.13 tokens per second)
      total time =      52.34 ms /     9 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
Set GGUF=/path/to/gpt-oss-20b-*.gguf
slot launch_slot_: id  0 | task 95 | processing task
slot update_slots: id  0 | task 95 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 1
slot update_slots: id  0 | task 95 | SWA checkpoint erase, pos_min = 2312, pos_max = 3072, size = 17.845 MiB
slot update_slots: id  0 | task 95 | SWA checkpoint erase, pos_min = 2305, pos_max = 3072, size = 18.009 MiB
slot update_slots: id  0 | task 95 | SWA checkpoint erase, pos_min = 1288, pos_max = 2048, size = 17.845 MiB
slot update_slots: id  0 | task 95 | kv cache rm [0, end)
slot update_slots: id  0 | task 95 | prompt processing progress, n_past = 1, n_tokens = 1, progress = 1.000000
slot update_slots: id  0 | task 95 | prompt done, n_past = 1, n_tokens = 1
slot update_slots: id  0 | task 95 | SWA checkpoint create, pos_min = 0, pos_max = 0, size = 0.024 MiB, total = 1/3 (0.024 MiB)
slot      release: id  0 | task 95 | stop processing: n_past = 8, truncated = 0
slot print_timing: id  0 | task 95 | 
prompt eval time =      18.03 ms /     1 tokens (   18.03 ms per token,    55.45 tokens per second)
       eval time =      37.11 ms /     8 tokens (    4.64 ms per token,   215.59 tokens per second)
      total time =      55.14 ms /     9 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
slot launch_slot_: id  0 | task 104 | processing task
slot update_slots: id  0 | task 104 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 2049
slot update_slots: id  0 | task 104 | kv cache rm [0, end)
slot update_slots: id  0 | task 104 | prompt processing progress, n_past = 1024, n_tokens = 1024, progress = 0.499756
slot update_slots: id  0 | task 104 | kv cache rm [1024, end)
slot update_slots: id  0 | task 104 | prompt processing progress, n_past = 2048, n_tokens = 1024, progress = 0.999512
slot update_slots: id  0 | task 104 | kv cache rm [2048, end)
slot update_slots: id  0 | task 104 | prompt processing progress, n_past = 2049, n_tokens = 1, progress = 1.000000
slot update_slots: id  0 | task 104 | prompt done, n_past = 2049, n_tokens = 1
slot update_slots: id  0 | task 104 | SWA checkpoint create, pos_min = 1281, pos_max = 2048, size = 18.009 MiB, total = 2/3 (18.033 MiB)
slot      release: id  0 | task 104 | stop processing: n_past = 2056, truncated = 0
slot print_timing: id  0 | task 104 | 
prompt eval time =     255.45 ms /  2049 tokens (    0.12 ms per token,  8021.11 tokens per second)
       eval time =      37.77 ms /     8 tokens (    4.72 ms per token,   211.79 tokens per second)
      total time =     293.23 ms /  2057 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
slot launch_slot_: id  0 | task 115 | processing task
slot update_slots: id  0 | task 115 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 2049
slot update_slots: id  0 | task 115 | need to evaluate at least 1 token for each active slot, n_past = 2049, n_prompt_tokens = 2049
slot update_slots: id  0 | task 115 | kv cache rm [2048, end)
slot update_slots: id  0 | task 115 | prompt processing progress, n_past = 2049, n_tokens = 1, progress = 0.000488
slot update_slots: id  0 | task 115 | prompt done, n_past = 2049, n_tokens = 1
slot update_slots: id  0 | task 115 | SWA checkpoint create, pos_min = 1288, pos_max = 2048, size = 17.845 MiB, total = 3/3 (35.878 MiB)
slot      release: id  0 | task 115 | stop processing: n_past = 2056, truncated = 0
slot print_timing: id  0 | task 115 | 
prompt eval time =      10.14 ms /     1 tokens (   10.14 ms per token,    98.61 tokens per second)
       eval time =      37.72 ms /     8 tokens (    4.72 ms per token,   212.07 tokens per second)
      total time =      47.86 ms /     9 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
slot launch_slot_: id  0 | task 124 | processing task
slot update_slots: id  0 | task 124 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 2049
slot update_slots: id  0 | task 124 | need to evaluate at least 1 token for each active slot, n_past = 2049, n_prompt_tokens = 2049
slot update_slots: id  0 | task 124 | kv cache rm [2048, end)
slot update_slots: id  0 | task 124 | prompt processing progress, n_past = 2049, n_tokens = 1, progress = 0.000488
slot update_slots: id  0 | task 124 | prompt done, n_past = 2049, n_tokens = 1
slot update_slots: id  0 | task 124 | SWA checkpoint erase, pos_min = 1288, pos_max = 2048, size = 17.845 MiB
slot update_slots: id  0 | task 124 | SWA checkpoint create, pos_min = 1288, pos_max = 2048, size = 17.845 MiB, total = 3/3 (53.699 MiB)
slot      release: id  0 | task 124 | stop processing: n_past = 2056, truncated = 0
slot print_timing: id  0 | task 124 | 
prompt eval time =       9.94 ms /     1 tokens (    9.94 ms per token,   100.60 tokens per second)
       eval time =      39.10 ms /     8 tokens (    4.89 ms per token,   204.62 tokens per second)
      total time =      49.04 ms /     9 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
slot launch_slot_: id  0 | task 133 | processing task
slot update_slots: id  0 | task 133 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 2049
slot update_slots: id  0 | task 133 | need to evaluate at least 1 token for each active slot, n_past = 2049, n_prompt_tokens = 2049
slot update_slots: id  0 | task 133 | kv cache rm [2048, end)
slot update_slots: id  0 | task 133 | prompt processing progress, n_past = 2049, n_tokens = 1, progress = 0.000488
slot update_slots: id  0 | task 133 | prompt done, n_past = 2049, n_tokens = 1
slot update_slots: id  0 | task 133 | SWA checkpoint erase, pos_min = 1288, pos_max = 2048, size = 17.845 MiB
slot update_slots: id  0 | task 133 | SWA checkpoint create, pos_min = 1288, pos_max = 2048, size = 17.845 MiB, total = 3/3 (53.535 MiB)
slot      release: id  0 | task 133 | stop processing: n_past = 2056, truncated = 0
slot print_timing: id  0 | task 133 | 
prompt eval time =       9.19 ms /     1 tokens (    9.19 ms per token,   108.83 tokens per second)
       eval time =      38.88 ms /     8 tokens (    4.86 ms per token,   205.75 tokens per second)
      total time =      48.07 ms /     9 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
slot launch_slot_: id  0 | task 142 | processing task
slot update_slots: id  0 | task 142 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 4097
slot      release: id  0 | task 142 | stop processing: n_past = 0, truncated = 0
srv    send_error: task id = 142, error: the request exceeds the available context size. try increasing the context size or enable context shift
srv  update_slots: no tokens to decode
srv  update_slots: all slots are idle
srv  cancel_tasks: cancel task, id_task = 142
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 400
slot launch_slot_: id  0 | task 145 | processing task
slot update_slots: id  0 | task 145 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 2049
slot update_slots: id  0 | task 145 | need to evaluate at least 1 token for each active slot, n_past = 2049, n_prompt_tokens = 2049
slot update_slots: id  0 | task 145 | kv cache rm [2048, end)
slot update_slots: id  0 | task 145 | prompt processing progress, n_past = 2049, n_tokens = 1, progress = 0.000488
slot update_slots: id  0 | task 145 | prompt done, n_past = 2049, n_tokens = 1
slot update_slots: id  0 | task 145 | SWA checkpoint erase, pos_min = 1288, pos_max = 2048, size = 17.845 MiB
slot update_slots: id  0 | task 145 | SWA checkpoint create, pos_min = 1288, pos_max = 2048, size = 17.845 MiB, total = 3/3 (53.535 MiB)
slot      release: id  0 | task 145 | stop processing: n_past = 2056, truncated = 0
slot print_timing: id  0 | task 145 | 
prompt eval time =      24.76 ms /     1 tokens (   24.76 ms per token,    40.38 tokens per second)
       eval time =      41.25 ms /     8 tokens (    5.16 ms per token,   193.93 tokens per second)
      total time =      66.02 ms /     9 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
slot launch_slot_: id  0 | task 154 | processing task
slot update_slots: id  0 | task 154 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 2049
slot update_slots: id  0 | task 154 | need to evaluate at least 1 token for each active slot, n_past = 2049, n_prompt_tokens = 2049
slot update_slots: id  0 | task 154 | kv cache rm [2048, end)
slot update_slots: id  0 | task 154 | prompt processing progress, n_past = 2049, n_tokens = 1, progress = 0.000488
slot update_slots: id  0 | task 154 | prompt done, n_past = 2049, n_tokens = 1
slot update_slots: id  0 | task 154 | SWA checkpoint erase, pos_min = 1288, pos_max = 2048, size = 17.845 MiB
slot update_slots: id  0 | task 154 | SWA checkpoint create, pos_min = 1288, pos_max = 2048, size = 17.845 MiB, total = 3/3 (53.535 MiB)
slot      release: id  0 | task 154 | stop processing: n_past = 2056, truncated = 0
slot print_timing: id  0 | task 154 | 
prompt eval time =       8.88 ms /     1 tokens (    8.88 ms per token,   112.65 tokens per second)
       eval time =      41.34 ms /     8 tokens (    5.17 ms per token,   193.53 tokens per second)
      total time =      50.21 ms /     9 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
slot launch_slot_: id  0 | task 163 | processing task
slot update_slots: id  0 | task 163 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 2049
slot update_slots: id  0 | task 163 | need to evaluate at least 1 token for each active slot, n_past = 2049, n_prompt_tokens = 2049
slot update_slots: id  0 | task 163 | kv cache rm [2048, end)
slot update_slots: id  0 | task 163 | prompt processing progress, n_past = 2049, n_tokens = 1, progress = 0.000488
slot update_slots: id  0 | task 163 | prompt done, n_past = 2049, n_tokens = 1
slot update_slots: id  0 | task 163 | SWA checkpoint erase, pos_min = 1288, pos_max = 2048, size = 17.845 MiB
slot update_slots: id  0 | task 163 | SWA checkpoint create, pos_min = 1288, pos_max = 2048, size = 17.845 MiB, total = 3/3 (53.535 MiB)
slot      release: id  0 | task 163 | stop processing: n_past = 2056, truncated = 0
slot print_timing: id  0 | task 163 | 
prompt eval time =       9.17 ms /     1 tokens (    9.17 ms per token,   109.08 tokens per second)
       eval time =      40.27 ms /     8 tokens (    5.03 ms per token,   198.68 tokens per second)
      total time =      49.43 ms /     9 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
slot launch_slot_: id  0 | task 172 | processing task
slot update_slots: id  0 | task 172 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 2049
slot update_slots: id  0 | task 172 | need to evaluate at least 1 token for each active slot, n_past = 2049, n_prompt_tokens = 2049
slot update_slots: id  0 | task 172 | kv cache rm [2048, end)
slot update_slots: id  0 | task 172 | prompt processing progress, n_past = 2049, n_tokens = 1, progress = 0.000488
slot update_slots: id  0 | task 172 | prompt done, n_past = 2049, n_tokens = 1
slot update_slots: id  0 | task 172 | SWA checkpoint erase, pos_min = 1288, pos_max = 2048, size = 17.845 MiB
slot update_slots: id  0 | task 172 | SWA checkpoint create, pos_min = 1288, pos_max = 2048, size = 17.845 MiB, total = 3/3 (53.535 MiB)
slot      release: id  0 | task 172 | stop processing: n_past = 2056, truncated = 0
slot print_timing: id  0 | task 172 | 
prompt eval time =       9.52 ms /     1 tokens (    9.52 ms per token,   105.02 tokens per second)
       eval time =      38.95 ms /     8 tokens (    4.87 ms per token,   205.39 tokens per second)
      total time =      48.47 ms /     9 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
slot launch_slot_: id  0 | task 181 | processing task
slot update_slots: id  0 | task 181 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 3073
slot update_slots: id  0 | task 181 | kv cache rm [2048, end)
slot update_slots: id  0 | task 181 | prompt processing progress, n_past = 3072, n_tokens = 1024, progress = 0.333225
slot update_slots: id  0 | task 181 | kv cache rm [3072, end)
slot update_slots: id  0 | task 181 | prompt processing progress, n_past = 3073, n_tokens = 1, progress = 0.333550
slot update_slots: id  0 | task 181 | prompt done, n_past = 3073, n_tokens = 1
slot update_slots: id  0 | task 181 | SWA checkpoint erase, pos_min = 1288, pos_max = 2048, size = 17.845 MiB
slot update_slots: id  0 | task 181 | SWA checkpoint create, pos_min = 2305, pos_max = 3072, size = 18.009 MiB, total = 3/3 (53.699 MiB)
slot      release: id  0 | task 181 | stop processing: n_past = 3080, truncated = 0
slot print_timing: id  0 | task 181 | 
prompt eval time =     144.53 ms /  1025 tokens (    0.14 ms per token,  7091.71 tokens per second)
       eval time =      42.78 ms /     8 tokens (    5.35 ms per token,   187.02 tokens per second)
      total time =     187.31 ms /  1033 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
slot launch_slot_: id  0 | task 191 | processing task
slot update_slots: id  0 | task 191 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 3073
slot update_slots: id  0 | task 191 | need to evaluate at least 1 token for each active slot, n_past = 3073, n_prompt_tokens = 3073
slot update_slots: id  0 | task 191 | kv cache rm [3072, end)
slot update_slots: id  0 | task 191 | prompt processing progress, n_past = 3073, n_tokens = 1, progress = 0.000325
slot update_slots: id  0 | task 191 | prompt done, n_past = 3073, n_tokens = 1
slot update_slots: id  0 | task 191 | SWA checkpoint erase, pos_min = 2305, pos_max = 3072, size = 18.009 MiB
slot update_slots: id  0 | task 191 | SWA checkpoint create, pos_min = 2312, pos_max = 3072, size = 17.845 MiB, total = 3/3 (53.699 MiB)
slot      release: id  0 | task 191 | stop processing: n_past = 3080, truncated = 0
slot print_timing: id  0 | task 191 | 
prompt eval time =       9.05 ms /     1 tokens (    9.05 ms per token,   110.55 tokens per second)
       eval time =      40.94 ms /     8 tokens (    5.12 ms per token,   195.41 tokens per second)
      total time =      49.98 ms /     9 tokens
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /completion 127.0.0.1 200
